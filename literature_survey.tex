\section{Related Work}
A lot of work has happened in the domain of the query planning of which the join ordering and join strategy are key components. It is agreed upon that the join reordering problem, in particular, is NP-Hard. There are quite a few approximation algorithms that try to solve this problem by leveraging the fact that not all join combinations are valid. System R \cite{b1} first suggested the use of Dynamic Programming (DP) for join ordering and is widely used by the join optimizers. DP based approach like System R come up with all possible valid join combinations and then chooses the plan with minimum cost. Guido and Thomas \cite{b3} propose a graph based dynamic programming strategy by avoiding the combinations which will not lead to valid join trees. The cost function for estimating the cost of running a given plan for both the approach relies heavily on column level statistics like cardinalities, and distinct values for defining the cost function.

The DP based approaches are not scalable as they have exponential run time as well as space complexity. So these techniques are only feasible when the number of joins is small. Spark for example, only reorders joins involving less than 12 relations. Large ad-hoc queries involving more than 10 relations are common in modern-day online analytical processing (OLTP) and object-oriented databases as the SQL queries are often nested. To tackle this problem,  the literature described in [\cite{b4}, \cite{b5}, \cite{b6}] proposes a heuristics-based approach for finding the optimal join order. These systems build the joins greedily either from a bottom-up or a top-down manner. This is different compared to the DP based approaches that rather than minimizing the cost of the entire plan they try to minimize the cost at each step. All though, this may not always produce the most optimal order, the primary goal here is to find a more optimal order compared to the current order.

Both DP and Greedy approached rely on the column level statistic and getting column-level statistics like cardinalities and distinct values require a full table scan.
This makes collecting and maintaining the stats very expensive. [Insert number of cases where stats are not available]. Even if the statistics are available, operators between the scan of the table and join can alter the data making the statistics inaccurate.
Adaptive query execution removes the hard dependency of collecting the up-front statistics and the assumption of their accuracy by reordering the query plan on the fly rather than doing it at the start. In batched query systems like Spark, adaptive collects the statistics after executing all substage below the shuffle boundary.
The newly generated statistics are then used for query replanning on the fly. This procedure is recursively applied at each shuffle boundary to leverage the newly updated statistics. Literature \cite{b7} reorders the nested loop joins after executing every batch of rows from the driver table based on the match rate of the joins. This approach however does not work for other join types because all the tables scans start in parallel. The  RIOS \cite{b8} takes a blocking approach where it starts by executing the pre-shuffle stages and then samples the data thus generated to get the information on cardinality and unique keys. Even though this approach is generic for all join types, the overhead of the sampling the data at shuffle stage and making blocking execution makes this approach infeasible in production environments. [Add numbers]
