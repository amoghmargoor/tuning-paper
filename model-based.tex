\section{Model Based Approach}

In the previous section, we saw that the iterative approach is not scalable as it is expensive to run jobs multiple times for optimizing. In this section, we will propose an alternative approach of using mathematical models to recommend new settings. In this approach, the cost of a query is modeled as a function of the configuration parameters. The relationship between cost and the parameters can be quite complex. To make it tractable, we apply parameter reduction as in the iterative approach~\ref{sec:paramreduction}. Earlier work~\cite{VanKen} on auto tuning Databases like MySQL and PostGres has also found that tuning only few knobs can improve performance significantly. We used this finding to choose few parameters that will impact performance significantly. 

We studied the effect of each parameter on the query cost by performing a set of experiments. Based on these results, we came up with some key insights that lead us to develop simple models for Big Data SQL Engines. We first present these experiments and insights in the earlier part of this section. In the latter part, we present a model for HIVE on MR Engine which can be used to recommend optimal settings. We present a detailed experimental evaluation of our model in the following section.

\subsection{Experiments and Key Insights}

The first insight is based on the experiments we did for the iterative approach. In those experiments, the metric optimized was the resource utilization (Formula~\ref{}), which depends on the memory used and the time taken by each task. If the memory used by each task (mapper/reducer/spark task) is reduced, the resource cost reduces. In practice, this saving in memory would be useful only if it could be gainfully used by some other task or job. However, for all the big data SQL engines, the memory used by all the task processes is the same. It is controlled by parameters at the query level (e.g. mapper memory, reducer memory, spark executor memory)and cannot be tuned per task. Further, the containers are allocated using Yarn, which allocates based on two dimensions - \textbf{memory} and \textbf{vcpu}. Each container is given 1 vcpu and some memory. If all the vcpus on a node are allocated and memory is still available, the extra memory cannot be used and is wasted. Conversely, if all memory is used up and extra vcpus are available, those extra vcpus are wasted since they cannot be allocated. This leads to the following conclusion:
\begin{insight}
	For optimal usage of both memory and vcpus, the memory/vcpu ratio of the containers should match the memory/vcpu ratio of the machine type.
\end{insight}
Figure \ref{fig:container_shape} illustrates the three scenarios:
\begin{enumerate}[label=(\alph*)]
	\item[$\bullet$] Memory/vCPU ratio of container is same as that of instance; hence, neither memory nor CPU is wasted. 
	\item[$\bullet$] Memory/vCPU ratio of container is smaller than that of instance; hence, memory is wasted as extra memory of instance is still remaining unused after container allocation.
	\item[$\bullet$] Memory/vCPU ratio of container is higher than that of instance; hence, CPU is wasted as extra CPU of instance is remaining unused after container allocation.
\end{enumerate}
This implies that memory for each container should be set according of the Memory/vCPU ratio of the machine. Thus, the memory configuration parameter (mapper memory, reducer memory or spark executor memory) need not be optimized per query and is based on the machine characteristics. Further, since memory per container is fixed, the target metric to be optimized should be the running time of the query rather than the resource utilization. We focus on this metric in the rest of the paper.


For each parameter of interest, we ran a set of experiments where we varied that parameter, while keeping the other parameters fixed and studied the effect on the query execution cost. 

We have run multiple experiments to optimize real world SQL workloads of our customers. This helped us to gain few key insights that we helped us to come with simple model. Following are those insights:

\noindent\subsubsection*{\bf Container shape should match that of Instance shape}
Yarn allocates containers on two dimensions - \textbf{memory} and \textbf{vcpu}. Each container is given 1 vcpu and some memory. The memory/vcpu of the containers should match the memory/vcpu ratio of the machine type. Otherwise resources are wasted. Figure \ref{fig:container_shape} illustrates three scenarios:
\begin{enumerate}[label=(\alph*)]
\item[$\bullet$] Memory/vCPU ratio of container is same as that of instance; hence, neither memory nor CPU is wasted. 
\item[$\bullet$] Memory/vCPU ratio of container is smaller than that of instance; hence, memory is wasted as extra memory of instance is still remaining unused after container allocation.
\item[$\bullet$] Memory/vCPU ratio of container is higher than that of instance; hence, CPU is wasted as extra CPU of instance is remaining unused after container allocation.
\end{enumerate}

\begin{figure*}[h]
	\includegraphics[width=0.33\linewidth]{container_shape1.png} 
	\includegraphics[width=0.33\linewidth]{container_shape2.png}
	\includegraphics[width=0.33\linewidth]{container_shape3.png}
	%\vspace*{-15pt}
	\caption{\textbf{(a)} No Resource is wasted \textbf{(b)} Memory is wasted \textbf{(c)} CPU is wasted}
	\label{fig:container_shape}
\end{figure*}

\noindent\subsubsection*{\bf Avoid Data Spills at all cost}
Data Spills happen when the buffer allotted is not enough to hold data and it's written to disk. Spills are expensive because each spill leads to an extra write and read of the data. In our experiments, we have found spills to add major overhead and should be avoided at all cost. Spills can be avoided by providing adequate memory to each task or by by making more fine grained tasks.
For example in Map Reduce, mapper spills can be avoided either by increasing Mapper container size or by reducing split size so that mapper tasks would process lesser data which might avoid spill.

\noindent\subsubsection*{\bf Decrease memory per task to choose a cheaper instance type}
On cloud platforms, machines with higher memory/cpu are more expensive for the same CPU type. Decrease the memory per task and consequently increase parallelism to choose a cheaper instance type. As long as tasks do not spill, the total work done in terms of IO, CPU and network traffic is independent of the parallelism factor of the tasks. For example, the total data read and processed will be the same if the number of mappers is 100 or 1000.  If a job can be tuned to avoid spills on a cheaper instance with same compute but lesser memory than original instance, then it is generally a good idea to move to cheaper instance for saving cost without any performance degradation. 

\noindent\subsubsection*{\bf Beware of secondary effects of high parallelism}
On the other hand, parallelism cannot be increased indefinitely. There are secondary effects of increasing the number of tasks. For example every task has to pay the cost of JVM start if applicable. Also there is an increase in number of communication channels. Thus parallelism should be not be set so high that secondary effects drown the increase in performance. This limit is specific to a workload or query and cluster configuration and can be determined algorithmically. 

\noindent\subsubsection*{\bf For Spark, prefer fat executors}
This insight is specific to Spark, where there is an additional parameter of cores per executor. Given a certain number of cores per machine, we have a choice of either running many executors with fewer cores per executor (thin executors), or fewer executors with more cores per executor (fat executors). We have observed that for Spark, fat executors generally provide better performance. This is because of several reasons such as  better memory utilization across cores in a executor, reduced number of replicas of broadcast tables and lesser overheads due to more tasks running in the same executor process.


\subsection{Model}

In this part of section, we will propose a model for Hive on Hadoop Map Reduce Job. We would propose a novel algorithm to optimize Hive Job for a couple of cost metrics. Firstly, we will propose algorithm to optimize cumulative time for Hive Job on a particular instance i.e., optimize metric $\mathcal{T} = \sum_{i=1}^{n} t_i$ where $n$ is number of containers and $t_i$ is execution time for $i^{th}$ container. Secondly, we would propose a straightforward algorithm to optimize cost which would use the first algorithm. In earlier work \cite{VanKen} on auto tuning Databases like MySQL and PostGres it is found that tuning only few knobs can improve performance significantly. We used this finding to choose few parameters that will impact performance significantly.
Following are the parameters of the model which would be used in the first algorithm:
\begin{enumerate}
    \item[$\bullet$] Job Parameters: There are the input parameters of the job that needs to be specified. Table \ref{table:job_params} defines these job parameters.
    \item[$\bullet$] Instance configuration: Table \ref{table:inst_conf} defines the machine configuration.
    \item[$\bullet$] Global Parameters: These are some of the global parameters that can be used to tune the algorithm. These are defined in Table \ref{table:global_params}.
\end{enumerate}

\begin{table}
\begin{tabular}{ |l|p {4.5 cm}| } 
 \hline
 Parameters & Description \\ 
 \hline
 mapperTime  & Total mapper time in seconds   \\ 
 numOfMapper & Number of map tasks \\ 
 mapperMemory & Container memory for map tasks  \\ 
 splitSize & Input Split Size \\
 mapperOutputBytes & Map output in bytes \\
 mapperOutputRecords & Number of Map output records \\
 reducerTime & Total reducer time in seconds \\
 numOfReducer & Number of reduce tasks \\
 bytesPerReducer & Corresponds to Hive parameter \textit{hive.exec.reducers.bytes.per.reducer} \\
 reducerMemory & Container memory for reduce tasks \\
 ioSort & Total amount of buffer memory in mega bytes to be used for sorting. Corresponds to Hadoop parameter \textit{mapreduce.task.io.sort.mb} \\
 spilledMapRecords & Number of records spilled in Map tasks  \\
 \hline
\end{tabular}
\caption{Parameters of the Job to be optimized}
\label{table:job_params}
\end{table}

\begin{table}
\begin{tabular}{ |l|p {4.5 cm}| }
 \hline
 Parameters & Description \\ 
 \hline
 nodeMemory  & Total available memory per node for MR job   \\ 
 cpuPerNode & Number of CPUs per node \\ 
 vCpuPerNode & Number of vCPUs per node  \\ 
 \hline
\end{tabular}
\caption{Instance configuration}
\label{table:inst_conf}
\end{table}

\begin{table}
\begin{tabular}{ |p {1.5 cm}|p {3.5 cm}|p {1 cm} | } 
 \hline
 Parameters & Description & Default\\ 
 \hline
 ioSortFrac & Size of ioSort buffer specified as fraction of mapper memory & 0.4 \\
 maxIOSort & Maximum value for \textit{mapreduce.task.io.sort.mb} & 2047 \\
 \hline
\end{tabular}
\caption{Global Parameters}
\label{table:global_params}
\end{table}

\noindent\subsubsection*{\bf Algorithm to optimize cumulative execution time for Hive on MR Job:}
In this part, we would propose Algorithm \ref{jobfit} to optimize cumulative execution time of HIVE on MR Job  i.e., metric $\mathcal{T}$ as defined above. It takes job parameters of the job and the instance configuration on which it was collected upon. Algorithm would then try to optimize metric $\mathcal{T}$ for the new instance configuration which is the third input to the algorithm. Fourth input is Global Parameters defined in Section \ref{subsubsec:global_param} which can be tuned based on the kind of workload.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}
\caption{fitJob}\label{jobfit}
\begin{algorithmic}[1]
\footnotesize
\REQUIRE  $\mathcal{P}$ is the job parameters (defined in Section \ref{table:job_params} ) to be optimized, $I_{old}$  is instance configuration (defined in Section \ref{table:inst_conf}) on which $\mathcal{P}$ is collected, $I_{new}$ is new instance configuration on which $\mathcal{P}$ needs to be optimized upon, $\mathcal{G}$ is the global parameters defined in Section \ref{table:global_params}
\ENSURE New Job parameter $\mathcal{P}_{new}$ that optimize the cumulative time of containers.
\IF {checkFit($\mathcal{P}, I_{new}$)} \RETURN $\mathcal{P}$ \COMMENT {checkFit is defined in Algorithm \ref{checkfit}}
\ENDIF
\STATE newMemPerCore $\gets I_{new}$.nodeMemory $/ I_{new}$.vCpuPerNode
\STATE $\mathcal{P}_{new}$.mapperMemory $\gets newMemPerCore$
\STATE $\mathcal{P}_{new}$.reducerMemory $\gets newMemPerCore$
\STATE ioSort $\gets$ newMemPerCore $\times$ ioSortFrac
\IF {ioSort $>$ $\mathcal{G}$.maxIOSort}
\STATE ioSort $\gets$ $\mathcal{G}$.maxIOSort
\ENDIF
\STATE $\mathcal{P}_{new}$.ioSort $\gets$ ioSort
\STATE newSplitSize $\gets$ $\mathcal{P}$.splitSize
\IF {spilled($\mathcal{P}$)}
\STATE outPerMap $\gets$ $\mathcal{P}$.mapperOutputBytes $/$ $\mathcal{P}$.numOfMapper
\STATE $newSplitSize \gets \mathcal{P}.splitSize \times \frac{ioSort}{outPerMap}$
\ELSIF {$\mathcal{P}.mapperMemory > newMemPerCore$}
\STATE $memRatio \gets \frac{\mathcal{P}.mapperMemory}{newMemPerCore}$
\STATE $newSplitSize \gets \frac{\mathcal{P}.splitSize}{memRatio} $
\ENDIF
\STATE $\mathcal{P}.splitSize \gets newSplitSize$
\IF {$\mathcal{P}.reducerMemory > newMemPerCore$}
\STATE $newBPR \gets \frac{\mathcal{P}.bytesPerReducer \times newMemPerCore}{\mathcal{P}.reducerMemory}$
\ELSE
\STATE $newBPR \gets \mathcal{P}.bytesPerReducer$
\ENDIF
\STATE $\mathcal{P}_{new}.bytesPerReducer \gets newBPR$
\STATE \RETURN $\mathcal{P}_{new}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{checkFit} \label{checkfit}
\begin{algorithmic}[1]
\footnotesize
\REQUIRE $\mathcal{P}$ is the job parameters (defined in \ref{table:job_params} ), $\mathcal{I}$  is instance configuration (defined in \ref{table:inst_conf})
\ENSURE Returns \textit{true} if $\mathcal{P}$ can fit into $\mathcal{I}$, otherwise \textit{false}.

\STATE newMemPerCore $\gets \mathcal{I}.nodeMemory / \mathcal{I}.vCpuPerNode$
\IF {newMemPerCore $> \mathcal{P}.mapperMemory$ and newMemPerCore $> \mathcal{P}.reducerMemory$}
\RETURN \textit{true}
\ELSE
\RETURN \textit{false}
\ENDIF
\end{algorithmic}
\end{algorithm}

\noindent\subsubsection*{\bf Algorithm to optimize cost metric:}

\begin{algorithm}
\caption{optimizeCost} \label{cost_optimize}
\begin{algorithmic}[1]
\footnotesize
\REQUIRE $\mathcal{P}$ is the job parameters (defined in \ref{table:job_params} ), Set $\mathcal{I} = {i_1, i_2, i_3, ...} $ set of instance configurations (defined in \ref{table:inst_conf}), Cost Metric Function $\mathcal{M}$
\ENSURE Returns optimized job parameter across all the instance configurations in $\mathcal{I}$.
\RETURN $\min_{\forall i \in \mathcal{I}} \mathcal{M}(fitJob(\mathcal{P}, i), i)$
\end{algorithmic}
\end{algorithm}

\subsection{Results}
We evaluated the effectiveness of model based method by running experiments on real workloads. The experiments were carried out for a HIVE on MR engine. To quantify the prediction error by the model, we ran an experiment on 4 queries of a customer. Figure \ref{fig:modelbasedresult} shows the benefit predicted by our model and the actual observed benefit for these queries. The actual savings closely match the predicted savings indicating that the model is sufficiently accurate.

\begin{figure*}[h]
	\includegraphics[width=\linewidth]{chart.png}
	\caption{Model Based Result: Predicted reduction in cost versus Actual reduction in cost}
	\label{fig:modelbasedresult}
\end{figure*}