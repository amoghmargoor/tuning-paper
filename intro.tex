\section{Introduction}

With the widespread adoption of cloud technologies, companies of all sizes are choosing to host their compute infrastructure on cloud platforms like AWS, Micrsoft Azure and Google Cloud Platform. 
Many of the best practices from in-house data center administration and tuning are also carried over to the cloud. However, cloud platforms have several unique properties like elasticity, separation of compute and storage, short lived clusters, and availability of different machine types. As big data infrastructure companies have matured, they are building systems that exploit some features of cloud platforms effectively. For example, Qubole provides auto-scaling 
and heterogenous clusters for Apache Hadoop, Apache Spark and PrestoDb systems. 

However, automatic workload tuning and capacity planning have not yet reached the same level of maturity as auto-scaling on cloud platforms. By tuning a workload, we mean determining a good set of configuration parameters and cluster settings for the queries to run efficiently. Organizations need to rely on experts to optimize big data systems for their workloads. However, the 
complexity of big data technologies and workloads combined with the choice provided by the flexibility of cloud platforms makes manual tuning unscalable. There are automated tools, but they are designed for static clusters and do not consider that more machines or different machine types can be used. \PDcomment{Can we site some tools here?}
Moreover, existing tools optimize
for performance only (since cost is fixed after provisioning hardware in the data center), whereas cost is also equally important in cloud deployments. 

In this paper, we address the problem of automatically determining good configuration parameters for SQL workloads with the goal of optimizing resource usage and cost. We present two alternatives -- an iterative method based on repeatedly executing queries with different configuration parameters and a model based method based on simple rules and insights. Both of these approaches have been proposed in some form in prior works. For example,~\cite{KumarPLPGB16} uses an simultaneous perturbation stochastic approximation (SPSA) based iterative algorithm to discover good configuration parameters. In the model based category, Starfish~\cite{herodotou2011profiling, herodotou2011starfish} was the one of the first systems to develop a comprehensive model for Hadoop map-reduce (MR) based systems. These previous works were directed towards generic workloads running on Hadoop systems, which increases the complexity of the problem due to the large number of possible configuration parameters and factors affecting system performance. This makes the iterative approach too expensive in practice due to large number of iterations. The model based approach, on the other hand, becomes very complex and brittle since they require the model to simulate every aspect of a data engine.

To mitigate this, we focus on SQL Workloads in our work. Though this may seem limiting, the fact is that SQL or SQL-like languages continue to be the most popular choice of ETL engineers and analysts. Since the mechanism of SQL query processing is well understood, we can use that knowledge to identify a smaller subset of configuration parameters that have significant impact on the query processing time and focus on optimizing them. 

We started with an iterative approach applying some more optimizations such as discretization, range reduction and dimension independence to further reduce the search space. Experiments on synthetic and real datasets show that the iterative algorithm was able to reduce resource utilization significantly (upto 80\%) compared to the default (for synthetic workloads) or expert chosen values (for real workloads) in a small number of iterations (usually around 10-15). However, the dollar cost of even the smaller number of iterations makes it practically infeasible.

We then moved to the model based approach that uses rules and heuristics to optimize performance and cost. Rules for performance optimization use data statistics from the catalog or previous runs. Additionally the rules use the machine type to factor in the resources available for a workload. The model optimizes cost by generating expected run types for all machine types and choosing the best combination of run time and price per machine. The goal of the model was not to predict the run time exactly, but rather to be able to accurately compare the relative performance of two sets of configuration parameters. The surprising result is that a very simple model is able to provide very good recommendations leading to significant savings in resource usage and cost. In fact, we found that these were often better than those chosen by experts on real customer workloads.

%Another important factor is the focus on SQL Workloads. Prior attempts at model based approach required models to simulate every aspect of a data engine. The advantage is that the approach is generic but the disadvantage is that it is complex and brittle. Prior research and our results show that a simple model is good enough for SQL Workloads. We compare the configuration values generated by the model with those determined by experts. Since experts did not choose optimal parameters, we determined optimal parameters using an iterative approach and compare with it as well.

The rest of the paper is organized as follows. Section~\ref{sec:relatedwork} gives an overview of the related work and Section~\ref{sec:optmethod} defines the objectives of the optimization process. Section~\ref{section:iter} describes the iterative approach and its results on real and synthetic datasets. Section~\ref{sec:insights} presents the insights that feed into developing the model based approach, which is detailed in Section~\ref{sec:modelbased} along with the experimental results. An extension of this approach for cloud platforms is presented in Section~\ref{sec:modelcloud}. We finally conclude in Section~\ref{sec:conclusions}.

\noindent\subsubsection*{Experimental Setup}
Qubole\cite{qubole} provides proprietary versions of Apache Hive\cite{thusoo2009hive}, Apache Spark\cite{zaharia2016apache} and Facebook Presto\cite{presto} to its customers.
All experiments were run on Apache Hive and Apache Spark using Qubole Data Service (QDS) on Amazon Web Services (AWS). 
The proprietary extensions on QDS do not affect the generality of the experiments.  
Due to budget and contract constraints, we have presented results from different customer workloads which consists of different data and queries. 
We have chosen Apache Spark as the representative engine for experiments. However a couple of customers run production workloads on Apache Hive executed on Hadoop 2 MR engine. To ensure consistency, we present results on a single engine type in each section or from both to show that the results are valid for both engines. 
A brief overview of the customers who provided permission to run experiments and their technology stack are:
\begin{enumerate}
	\item[$\bullet$] Customer 1: A large e-commerce company that uses Apache Hive executed on Hadoop2 MR engine to process click stream and product data. 
	\item[$\bullet$] Customer 2: A large travel logistics company that uses Apache Hive executed on Hadoop2 MR engine to process data on demand and usage of its services.
	\item[$\bullet$] Customer 3: A Platform-As-A-Service company that uses Apache Spark to process telemetry data from its platform.
\end{enumerate}

Section~\ref{section:iter} on iterative approach contains experimental results at Customer 1 using Apache Hive on Hadoop 2 MR engine. 
For this reason we also present results of the synthetic workload using Apache Hive on Hadoop 2 MR engine. 
In Section~\ref{sec:insights} and Section~\ref{sec:modelbased} we generate insights and build a model using Apache Spark. 
We ran experiments for model-based approach at both Customer 2 and Customer 3. 
Therefore we modified the model for Apache Hive on Hadoop and present results for both. In Section~\ref{sec:modelcloud} we present results from Customer 3 on Apache Spark only.   
