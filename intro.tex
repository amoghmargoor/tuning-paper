\section{Introduction}

With the widespread adoption of cloud technologies, companies of all sizes are choosing to host their compute infrastructure on cloud platforms like AWS, Micrsoft Azure and Google Cloud Platform. 
Many of the best practices from in-house data center administration and tuning are also carried over to the cloud. However, cloud platforms have several unique properties like elasticity, separation of compute and storage, short lived clusters, and availability of different machine types. With experience, engineers are building systems to exploit the unique properties
of cloud platforms, especially elasticity. 

These trends are applicable to big data workloads as well. As Big data infrastructure companies have matured, they are building features to make use of the elasticity of cloud platforms effectively. For example, Qubole provides auto-scaling 
and heterogenous clusters for Apache Hadoop, Apache Spark and PrestoDb clusters. 
However, automatic workload tuning and capacity planning have not yet reached the same level of maturity as auto-scaling on cloud platforms. By tuning a workload, we mean determining a good set of configuration parameters and cluster settings for the queries to run optimally. 

Organizations need to rely on experts to optimize the big data systems for their workloads. but the 
complexity of the big data technologies and workloads combined with elasticity is beyond abilities of humans. There are automated tools but they are 
designed for static clusters. The tools do not consider that more machines or different machines types can be used. Moreover all the tools optimize
for performance only (since cost is fixed after provisioning hardware in the data center) while in cloud deployments cost is also equally important. 

In this paper, we propose a model based approach that uses rules and heuristics to optimize performance and cost. Rules for performance optimization
use data statistics from the catalog or previous runs. Additionally the rules use the machine type to factor in the resources available for a workload.
The model optimizes cost by generating expected run types for all machine types and choosing the best combination of run time and price per machine.

Another important factor is the focus on SQL Workloads. Prior attempts at model based approach required models to simulate every aspect of a 
data engine. The advantage is that the approach is generic but the disadvantage is that it is complex and brittle. Prior research and our results
show that a simple model is good enough for SQL Workloads. We compare the configuration values generated by the model with those determined by experts.
Since experts did not choose optimal parameters, we determined optimal parameters using an iterative approach and compare with it as well.

The rest of the paper is organized as follows: Section 2. explores related work. Section 3. describes an Iterative approach that provides optimal 
configuration with higher confidence. Section 4. describes the Model-Based approach. Section 5. is the Experimental Results.
