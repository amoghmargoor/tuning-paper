\section{Introduction}

With the widespread adoption of cloud technologies, companies of all sizes are choosing to host their compute infrastructure on cloud platforms like AWS, Micrsoft Azure and Google Cloud Platform. 
Many of the best practices from in-house data center administration and tuning are also carried over to the cloud. However, cloud platforms have several unique properties like elasticity, separation of compute and storage, short lived clusters, and availability of different machine types. As big data infrastructure companies have matured, they are building systems that exploit some features of cloud platforms effectively. For example, Qubole provides auto-scaling 
and heterogenous clusters for Apache Hadoop, Apache Spark and PrestoDb systems. 

However, automatic workload tuning and capacity planning have not yet reached the same level of maturity as auto-scaling on cloud platforms. By tuning a workload, we mean determining a good set of configuration parameters and cluster settings for the queries to run efficiently. Organizations need to rely on experts to optimize big data systems for their workloads. However, the 
complexity of big data technologies and workloads combined with the choice provided by the flexibility of cloud platforms makes manual tuning unscalable. 

In this paper, we address the problem of automatically determining good configuration parameters for SQL workloads with the goal of optimizing resource usage and cost. We studied two alternatives -- an iterative method based on repeatedly executing queries with different configuration parameters and a model based method that relies on simple rules and insights. Both of these approaches have been proposed in some form in prior works. For example,~\cite{KumarPLPGB16} uses a simultaneous perturbation stochastic approximation (SPSA) based iterative algorithm to discover good configuration parameters. In the model based category, Starfish~\cite{herodotou2011profiling, herodotou2011starfish} was the one of the first systems to develop a comprehensive model for Hadoop map-reduce (MR) based systems. These previous works were directed towards generic workloads running on Hadoop systems, which increases the complexity of the problem due to the large number of possible configuration parameters and factors affecting system performance. This makes the iterative approach too expensive in practice due to large number of iterations. The model based approach, on the other hand, becomes very complex and brittle since they require the model to simulate every aspect of a data engine.

Our experience with customer workloads at Qubole suggests that SQL or SQL-like languages continue to be the most popular choice of ETL engineers and analysts. At Qubole, approximately 75\% of the workloads are expressed in SQL or SQL-like languages. Thus, we chose to focus on SQL Workloads in our work rather than generic workloads. Since the mechanism of SQL query processing is well understood, we can use that knowledge to identify a smaller subset of configuration parameters that have significant impact on the query processing time and focus on optimizing them. 


We started with an iterative approach applying some more optimizations such as discretization, range reduction and dimension independence to further reduce the search space. Experiments on synthetic and real datasets show that the iterative algorithm was able to reduce resource utilization significantly (upto 80\%) compared to the default (for synthetic workloads) and also expert chosen values (for real workloads) in a small number of iterations (usually around 10-15). However, the dollar cost of even the smaller number of iterations makes it practically infeasible. For example, the iterative algorithm took over 50 hours and cost \$5000 for a real customer workload. Due to space constraints, we have omitted the details of the iterative approach in this paper and made them available in the full version instead~\cite{fullpaper}.

We then moved to the model based approach that uses rules and heuristics to optimize performance and cost. Rules for performance optimization use data statistics from the catalog or from previous runs. We further extended the model for cloud platforms by adding rules that use machine type to factor in the resources available for a workload. The model optimizes cost by computing expected run times for all machine types and choosing the best combination of run time and price per machine. The goal of the model was not to predict the run time exactly, but rather to be able to accurately compare the relative performance of two sets of configuration parameters. The encouraging result is that a very simple rule based model is able to provide very good recommendations leading to significant savings in resource usage and cost. \textit{In fact, we found that these were often better than those chosen by experts on real customer workloads.}

The rest of the paper is organized as follows. Section~\ref{sec:relatedwork} gives an overview of the related work and Section~\ref{sec:optmethod} defines the objectives of the optimization process. \eat{Section~\ref{section:iter} describes the iterative approach and its results on real and synthetic datasets.} Section~\ref{sec:insights} presents the insights that feed into developing the model based approach, which is detailed in Section~\ref{sec:modelbased} along with the experimental results. An extension of this approach for cloud platforms is presented in Section~\ref{sec:modelcloud}. We finally conclude in Section~\ref{sec:conclusions}.

\noindent\subsubsection*{Experimental Setup}
Qubole\cite{qubole} provides proprietary versions of Apache Hive\cite{thusoo2009hive}, Apache Spark\cite{zaharia2016apache} and Facebook Presto\cite{presto} to its customers.
All experiments were run on Apache Hive and Apache Spark using Qubole Data Service (QDS) on Amazon Web Services (AWS). 
The models and experiments do not use any proprietary extensions of QDS.  
Due to budget and contract constraints, we have presented results from different customer workloads which consists of different data, queries, engine, cluster and machine configuration.
To ensure consistency, we present results for each approach (\eat{iterative,} model and cloud model) from a single company using a specific dataset, query set and data engine. 
Results for each approach are self-contained and independent. Moreover the variety of experiments and setup show the versatility of our approach. 

The paper contains experimental results from a subset of TPC-DS\cite{poess2002tpc} queries containing FILTER, GROUP BY, JOIN and SUBQUERIES. Cosmetic changes were made to run these
queries on Apache Hive and Apache Spark. The paper also contains results of queries run on real workloads. These queries were provided by customers and chosen from their ETL workloads. These queries are of similar complexity to TPC-xBB benchmark\cite{nambiar2014benchmarking} and include FILTER, GROUP BY, JOIN, SUBQUERIES and various types of UDFs.

A brief overview of the customers who provided permission to run experiments and their technology stack are:
\begin{enumerate}
	\item[$\bullet$] Customer 1: A large e-commerce company that uses Apache Hive executed on Hadoop2 MR engine to process click stream and product data. 
	\item[$\bullet$] Customer 2: A large travel logistics company that uses Apache Hive executed on Hadoop2 MR engine to process data on demand and usage of its services.
	\item[$\bullet$] Customer 3: A Platform-As-A-Service company that uses Apache Spark to process telemetry data from its platform.
\end{enumerate}
\eat{
Section~\ref{section:iter} on iterative approach contains experimental results at Customer 1 using Apache Hive on Hadoop 2 MR engine. 
For this reason we also present results of the synthetic workload using Apache Hive on Hadoop 2 MR engine. 
In Section~\ref{sec:insights} and Section~\ref{sec:modelbased} we generate insights and build a model using Apache Spark. 
We ran experiments for model-based approach at both Customer 2 and Customer 3. 
Therefore we modified the model for Apache Hive on Hadoop and present results for both. In Section~\ref{sec:modelcloud} we present results from Customer 3 on Apache Spark only.   
}
\eat{
We evaluated the iterative approach using the Customer 1 environment, but the results are omitted in this paper. In Section~\ref{sec:modelbased}, we used Customer 2 and Customer 3 environments to evaluate the model based approaches for Apache Hive on Hadoop and Apache Spark respectively. Finally, the cloud based model was evaluated only for Apache Spark using the Customer 3 environment, as detailed in Section~\ref{sec:modelcloud}.
}33