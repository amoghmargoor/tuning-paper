\section{Optimization Methodology}
\label{sec:optmethod}
We now present the optimization functions, parameters and assumptions to tune SQL-on-Hadoop data engines. Even though the model is specific to run time features of SQL-on-Hadoop engines, the principles can be translated to other engines. 

\subsection{Optimization Function}
The optimization function should consider both cost and performance of a workload. In this section, we develop a metric to capture cost and performance of SQL-on-Hadoop engines running on Hadoop2 Yarn Containers. Hadoop divides machines into containers and each container is assigned a fixed amount of memory and CPUs. Workloads are submitted to Hadoop by Hive or Spark as a set of tasks which are scheduled by Hadoop on containers. The first metric we considered measures the total resource utilization. Since memory and CPU are both important resources, the cumulative resource utilization is given by:
\begin{equation}
\label{eqn:totalresource}
\mathcal{R} = \sum_{i=1}^{n} t_i \times m_i
\end{equation}
In the above equation, $n$ is the number of tasks, $t_i$ is the execution time for task $i$ and $m_i$ is the memory allocated to the container on which the task is scheduled. The time factor in the equation tracks the performance of the query. In a cloud deployment, the dollar cost of running the workload becomes an important metric. If the workload under consideration is the only one running on the cluster, then the cumulative time that the workload is scheduled on all the containers is a good proxy for the cost of the query, as given by:
\begin{equation}
\label{eqn:totaltime}
\mathcal{T} = \sum_{i=1}^{n} t_i
\end{equation}
Different machine types on a cloud platform can have different monetary costs depending on the number of cores, amount of memory, storage and network speeds. The actual dollar cost of a workload taking a cumulative time $\mathcal{T}_x$ on a cluster consisting of machines of type $x$ having a rental rate of $r_x$ per core, per unit time is given by:
\begin{equation}
\label{eqn:totalcost}
\mathcal{C} = \mathcal{T}_x \times r_x
\end{equation}


\subsection{Parameters}


\begin{table*}
	\begin{tabular}{ |l|l|l| } 
		\hline
		Parameter & Hive on MR & Spark \\ 
		\hline
		Mapper memory & mapreduce.map.memory.mb & \multirow{2}{*}{spark.executor.memory} \\
		Reducer memory & mapreduce.reduce.memory.mb & \\
		\hline
		\multirow{2}{*}{Mapper parallelism} & \multicolumn{2}{|c|}{mapreduce.input.fileinputformat.split.maxsize} \\
		& \multicolumn{2}{|c|}{mapreduce.input.fileinputformat.split.minsize} \\
		\hline
		Reducer parallelism & hive.exec.reducers.bytes.per.reducer & spark.sql.shuffle.partitions \\
		\hline
		Executor cores & Not applicable & spark.executor.cores \\
		\hline
	\end{tabular}
	\caption{Parameters of the Job to be optimized}
	\label{table:job_params}
\end{table*}

\mycomment{
\begin{table}
\begin{tabular}{ |l|p {4.5 cm}| } 
 \hline
 Parameters & Description \\ 
 \hline
 mapperTime  & Total mapper time in seconds   \\ 
 numOfMapper & Number of map tasks \\ 
 mapperMemory & Container memory for map tasks  \\ 
 splitSize & Input Split Size \\
 mapperInputBytes & Map input in bytes \\
 mapperOutputBytes & Map output in bytes \\
 mapperOutputRecords & Number of Map output records \\
 reducerTime & Total reducer time in seconds \\
 numOfReducer & Number of reduce tasks \\
 bytesPerReducer & Corresponds to Hive parameter \textit{hive.exec.reducers.bytes.per.reducer} \\
 reducerMemory & Container memory for reduce tasks \\
 ioSort & Total amount of buffer memory in mega bytes to be used for sorting. Corresponds to Hadoop parameter \textit{mapreduce.task.io.sort.mb} \\
 spilledMapRecords & Number of records spilled in Map tasks  \\
 \hline
\end{tabular}
\caption{Parameters of the Job to be optimized}
\label{table:job_params}
\end{table}
}

\begin{table}[h]
\begin{tabular}{ |l|p {4.5 cm}| }
 \hline
 Parameters & Description \\ 
 \hline
 nodeMemory  & Total available memory per node for MR job   \\ 
 cpuPerNode & Number of CPUs per node \\ 
 vCpuPerNode & Number of vCPUs per node  \\ 
 \hline
\end{tabular}
\caption{Instance configuration}
\label{table:inst_conf}
\end{table}

\begin{table}[h]
\begin{tabular}{ |p {1.5 cm}|p {3.5 cm}|p {1 cm} | } 
 \hline
 Parameters & Description & Default\\ 
 \hline
 ioSortFrac & Size of ioSort buffer specified as fraction of mapper memory & 0.4 \\
 maxIOSort & Maximum value for \textit{mapreduce.task.io.sort.mb} & 2047 \\
 reducerFrac & Fraction of Reducer memory to be used as buffer & 0.4 \\ 
 \hline
\end{tabular}
\caption{Global Parameters}
\label{table:global_params}
\end{table}

From OtterTune\cite{vanaken}, a tool to auto tune Databases like MySQL and PostgreSQL, it was found that tuning only few knobs can improve performance significantly. This finding carries over to SQL-on-Hadoop engines as well. We chose a few critical parameters to optimize by consulting experts in the domain. The set parameters can be classified as follows:
\begin{enumerate}
	\item[$\bullet$] Job Parameters: These are the parameters that can be set for each job separately, can thus be tuned for each query in the workload. Table \ref{table:job_params} lists these job parameters for Map-reduce and Spark engine.
	\item[$\bullet$] Instance configuration: These parameters are determined by the machine type and are not tunable per query. Table \ref{table:inst_conf} lists the machine type based configuration.
	\item[$\bullet$] Global Parameters: These are some of the global parameters that are neither query nor machine dependent. They can be considered as constants and can be used to tune the algorithm. These are listed in Table \ref{table:global_params} for Map-reduce engines.
\end{enumerate}

%For Map-Reduce engine, the parameters are described in Tables 1, 2 and 3.

