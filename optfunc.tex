\section{Optimization Methodology}
\label{sec:optmethod}
We now present the optimization functions, parameters and assumptions to tune SQL-on-Hadoop data engines running on Hadoop2 Yarn Containers. \eat{Even though the model is specific to run time features of SQL-on-Hadoop engines, the principles can be translated to other engines. }

\subsection{Optimization Functions}
\eat{The optimization function should consider both cost and performance of a workload. In this section, we develop a metric to capture cost and performance of SQL-on-Hadoop engines running on Hadoop2 Yarn Containers.} Hadoop divides machines into containers and each container is assigned a fixed amount of memory and one CPU. \eat{Workloads are submitted to Hadoop by Hive or Spark as a set of tasks which are scheduled by Hadoop on containers. The first metric we consider measures the total resource utilization. Since memory and CPU are both important resources,} The cumulative resource utilization is given by:
\begin{equation}
\label{eqn:totalresource}
\mathcal{R} = \sum_{i=1}^{n} t_i \times m_i
\end{equation}
In the above equation, $n$ is the number of tasks, $t_i$ is the execution time for task $i$ and $m_i$ is the memory allocated to the container on which the task is scheduled. \eat{The time factor in the equation tracks the performance of the query.} In a cloud deployment, the dollar cost of running the workload becomes an important metric. \eat{If the workload under consideration is the only one running on the cluster, then the cumulative time that the workload is scheduled on all the containers is a good proxy for the cost of the query, as given by:} The cumulative CPU time of the query is a good proxy for the cost of the query, as given by:
\begin{equation}
\label{eqn:totaltime}
\mathcal{T} = \sum_{i=1}^{n} t_i
\end{equation}
\begin{equation}
\label{eqn:totalcost}
\mathcal{C} = \mathcal{T}_x \times r_x
\end{equation}


Different machine types on a cloud platform can have different monetary costs\eat{ depending on the number of cores, amount of memory, storage and network speeds}. The actual dollar cost of a workload taking a cumulative time $\mathcal{T}_x$ on a cluster consisting of machines of type $x$ having a rental rate of $r_x$ per core, per unit time is given by:
\subsection{Parameters}

\eat{
\begin{table*}
	\begin{center}
		\begin{tabular}{ |l|l|l| } 
			\hline
			Parameter & Hive on MR & Spark \\ 
			\hline
			Mapper memory & mapreduce.map.memory.mb & \multirow{2}{*}{spark.executor.memory} \\
			Reducer memory & mapreduce.reduce.memory.mb & \\
			\hline
			\multirow{2}{*}{Mapper parallelism} & \multicolumn{2}{|c|}{mapreduce.input.fileinputformat.split.maxsize} \\
			& \multicolumn{2}{|c|}{mapreduce.input.fileinputformat.split.minsize} \\
			\hline
			Reducer parallelism & hive.exec.reducers.bytes.per.reducer & spark.sql.shuffle.partitions \\
			\hline
			Executor cores & Not applicable & spark.executor.cores \\
			\hline
			Mapper Buffer & mapreduce.task.io.sort.mb & Not applicable \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Parameters of the Job to be optimized}
	\label{table:job_params}	
\end{table*}
}


From OtterTune\cite{vanaken}, a tool to auto tune Databases like MySQL and PostgreSQL, it was found that tuning only few knobs can improve performance significantly. This finding carries over to SQL-on-Hadoop engines as well. We chose a few critical parameters to optimize by consulting industry experts in the domain. These parameters can be set for each job separately and can thus be tuned for each query in the workload. Table \ref{table:job_params} lists these job parameters for Map-reduce and Spark engine. At the same time, the methodology is applicable even if the set is augmented with more parameters or if a different set of parameters are chosen. 

\begin{table}
	\begin{center}
		\begin{tabular}{ |p{1.3cm}|p{3.4cm}|p{2.8cm}| } 
			\hline
			Parameter & Hive on MR & Spark \\ 
			\hline
			Mapper memory & mapreduce.map. memory.mb & \multirow{2}{*}{spark.executor.memory} \\
			Reducer memory & mapreduce.reduce.memory.mb & \\
			\hline
			\multirow{2}{1cm}{Mapper parallelism} & \multicolumn{2}{|c|}{mapreduce.input.fileinputformat.split.maxsize} \\
			& \multicolumn{2}{|c|}{mapreduce.input.fileinputformat.split.minsize} \\
			\hline
			Reducer parallelism & hive.exec.reducers. bytes.per.reducer & spark.sql.shuffle.partitions \\
			\hline
			Executor cores & Not applicable & spark.executor.cores \\
			\hline
			Mapper Buffer & mapreduce.task.io.sort.mb & Not applicable \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Parameters of the Job to be optimized}
	\label{table:job_params}	
\end{table}

\eat{
The set of parameters can be classified as follows:
\begin{enumerate}
	\item[$\bullet$] Job Parameters: These are the parameters that can be set for each job separately, can thus be tuned for each query in the workload. Table \ref{table:job_params} lists these job parameters for Map-reduce and Spark engine.
	\item[$\bullet$] Instance configuration: These parameters are determined by the machine type and are not tunable per query. Table \ref{table:inst_conf} lists the machine type based configuration.
	\item[$\bullet$] Global Parameters: These are some of the global parameters that are neither query nor machine dependent. They can be considered as constants and can be used to tune the algorithm. These are listed in Table \ref{table:global_params} for Map-reduce engines.
\end{enumerate}
}
%For Map-Reduce engine, the parameters are described in Tables 1, 2 and 3.

