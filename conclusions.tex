\section{Conclusions}
\label{sec:conclusions}
In this paper, we have addressed the problem of determining good configuration parameters for SQL workloads on big data platforms, specifically SQL-On-Hadoop engines like Hive on MR and Spark SQL. We started with an iterative algorithm that executes each query multiple times with different configuration parameters. Though this method was able to discover good configurations within a small number (around 10-15) of iterations, we found that it was not feasible for real customers due to the large dollar cost of executing each query multiple times. We then focused on a model based approach and developed models for Hive and Spark SQL based on some insights gained through running many experiments. We further extended the model for cloud platforms that require recommending instance types for the cluster in addition to the other parameters. Our results show that the models were able to provide very good recommendations compared to the default and expert chosen configurations on real customer workloads. 

It may sound surprising that very simple models proved to be very effective in practice. This was possible since we chose to focus on SQL-On-Hadoop workloads rather than generic MR or Spark workloads. This enabled us to reason about the query execution and develop the insights. Future work includes extending the models for other engines like Tez and Presto, exploring more configuration parameters and applying the models on many more real workloads by integrating it with the Qubole product. 
